{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "VALI = 0.9\n",
    "X_PATH = '../../ML2017_data/bump/df_x.csv'\n",
    "Y_PATH = '../../ML2017_data/bump/df_y.csv'\n",
    "X_VAL_PATH = '../../ML2017_data/bump/df_x_val.csv'\n",
    "Y_VAL_PATH = '../../ML2017_data/bump/df_y_val.csv'\n",
    "X_TEST = '../../ML2017_data/bump/test_x.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_path = 'yx_x'\n",
    "train_y_path = 'yx_y'\n",
    "val_x_path = 'yx_x_val'\n",
    "val_y_path = 'yx_y_val'\n",
    "test_x_path = 'yx_x_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voter portion: 0.03\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(1425, 3)\n",
      "1425\n",
      "score: 0.816245791246\n",
      "voter portion: 0.06\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(2851, 3)\n",
      "2851\n",
      "score: 0.816919191919\n",
      "voter portion: 0.09\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(4276, 3)\n",
      "4276\n",
      "score: 0.817760942761\n",
      "voter portion: 0.12\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(5702, 3)\n",
      "5702\n",
      "score: 0.815656565657\n",
      "voter portion: 0.15\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(7128, 3)\n",
      "7128\n",
      "score: 0.815656565657\n",
      "voter portion: 0.18\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n",
      "finish xgb bagging\n",
      "(8553, 3)\n",
      "8553\n",
      "score: 0.814393939394\n",
      "voter portion: 0.21\n",
      "Finish normizatilon\n",
      "Finish label\n",
      "Finish random forest.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7fa24c644d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         clf = xgb.train(xgb_par, \n\u001b[1;32m     94\u001b[0m             \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mnum_boost_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NUM = 3\n",
    "VALI = 1 \n",
    "VOTER = 0.08\n",
    "val_score = []\n",
    "\n",
    "for VOTER in range(1,11):\n",
    "    VOTER = 0.03 * VOTER\n",
    "    print('voter portion:', VOTER)\n",
    "    x = np.array(pickle.load(open(train_x_path, 'rb')))\n",
    "    y = np.array(pickle.load(open(train_y_path, 'rb')))\n",
    "\n",
    "    randomize = np.arange(len(x))\n",
    "    x_test = pickle.load(open(test_x_path, 'rb'))\n",
    "    all_x = np.concatenate((x, x_test))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_x)\n",
    "    x = scaler.transform(x)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    pickle.dump(scaler, open('scaler', 'wb'))\n",
    "    print('Finish normizatilon')\n",
    "\n",
    "    label_encorder = LabelEncoder()\n",
    "    y = label_encorder.fit_transform(y)\n",
    "    pickle.dump(label_encorder, open('label_encoder', 'wb'))\n",
    "    print('Finish label')\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators = 1500,\n",
    "#                 bootstrap = True,\n",
    "#                 n_jobs = -1, \n",
    "#                 min_samples_split=7,\n",
    "#                 oob_score = True)\n",
    "#     clf.fit(x, y)\n",
    "#     score = clf.oob_score_\n",
    "#     test_ans = clf.predict_proba(x_test)\n",
    "#     ans = clf.predict(x_test)\n",
    "#     #     pickle.dump(clf, open('./model/{:.4f}'.format(score), 'wb'))\n",
    "#     train_l = np.where(test_ans > 0.65)\n",
    "#     #print(test_ans.shape)\n",
    "#     #print(x_test.shape)\n",
    "#     #print('ans shape', ans.shape)\n",
    "#     x_test = np.array(x_test)\n",
    "#     self_train_x = x_test[train_l[0]]\n",
    "#     self_train_y = ans[train_l[0]]\n",
    "#     x_test = np.delete(x_test, train_l[0], axis = 0)\n",
    "#     x = np.concatenate((x, self_train_x))\n",
    "#     y = np.concatenate((y, self_train_y))\n",
    "\n",
    "    randomize = np.arange(len(x))\n",
    "    np.random.shuffle(randomize)\n",
    "    x = x[randomize]\n",
    "    y = y[randomize]\n",
    "    VOTER = int(len(x)*VOTER)\n",
    "    x_model = x[VOTER:]\n",
    "    y_model = y[VOTER:]\n",
    "    x_voter = x[:VOTER]\n",
    "    y_voter = y[:VOTER]\n",
    "\n",
    "    rf_clf = RandomForestClassifier(n_estimators = 1500,\n",
    "                n_jobs = -1, \n",
    "                min_samples_split=10,\n",
    "                oob_score = True)\n",
    "    rf_clf.fit(x_model, y_model)\n",
    "    print('Finish random forest.')\n",
    "\n",
    "\n",
    "    for model_i in range(MODEL_NUM):\n",
    "        np.random.seed(model_i)\n",
    "        randomize = np.arange(len(x_model))\n",
    "        np.random.shuffle(randomize)\n",
    "        x_model = x_model[randomize]\n",
    "        y_model = y_model[randomize]\n",
    "        TRAIN = int(VALI * len(x_model))\n",
    "        x_t = x_model[:TRAIN]\n",
    "        y_t = y_model[:TRAIN]\n",
    "        x_v = x_model[TRAIN:]\n",
    "        y_v = y_model[TRAIN:]\n",
    "\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_t, y_t)\n",
    "        dval = xgb.DMatrix(x_v, y_v)\n",
    "\n",
    "        xgb_par = {\n",
    "                'objective' : 'multi:softprob',\n",
    "                'booster' : 'gbtree',\n",
    "                'eta' : 0.03,\n",
    "                'subsample' : 0.9,\n",
    "                'num_class' : 3,\n",
    "                'max_depth' : 15,\n",
    "                'colsample_bytree' : .6,\n",
    "                }\n",
    "\n",
    "        clf = xgb.train(xgb_par, \n",
    "            dtrain = dtrain,\n",
    "            num_boost_round = 300,\n",
    "            )\n",
    "\n",
    "        pickle.dump(clf, open('./stacking/' + str(model_i) + '_xgb', 'wb'))\n",
    "    print('finish xgb bagging')\n",
    "        \n",
    "    rf_pred = rf_clf.predict_proba(x_voter)\n",
    "    # rf_pred = rf_pred.reshape((rf_pred.shape[0], 1))\n",
    "    print(rf_pred.shape)\n",
    "\n",
    "    # svc = svm.SVC(probability=True)\n",
    "    # svc.fit(x, y)\n",
    "    # svc_pred = svc.predict_proba(x_val)\n",
    "    # all_pred = np.concatenate((rf_pred, svc_pred), axis = 1)\n",
    "    # print('Finish svc')\n",
    "\n",
    "    print(VOTER)\n",
    "\n",
    "    # dvoter = xgb.DMatrix(x_voter)\n",
    "    # xgb_pred = np.zeros((VOTER,3))\n",
    "    # for i in range(MODEL_NUM):\n",
    "    #     model = pickle.load(open('stacking/' + str(i) + '_xgb', 'rb'))\n",
    "    #     xgb_pred += model.predict(dvoter, ntree_limit = model.best_ntree_limit)\n",
    "    # xgb_pred /= MODEL_NUM\n",
    "    # print(rf_pred.shape)\n",
    "    # print('xgb_pred shape',np.array(xgb_pred).shape)\n",
    "    # all_pred = np.concatenate((all_pred,xgb_pred), axis = 1)\n",
    "    # print(all_pred[0])\n",
    "\n",
    "    dvoter = xgb.DMatrix(x_voter)\n",
    "    xgb_pred = []\n",
    "    for i in range(MODEL_NUM):\n",
    "        model = pickle.load(open('stacking/' + str(i) + '_xgb', 'rb'))\n",
    "        xgb_pred.append(model.predict(dvoter, ntree_limit = model.best_ntree_limit))\n",
    "    xgb_pred = np.concatenate(xgb_pred, axis = 1)\n",
    "    all_pred = np.concatenate((rf_pred,xgb_pred), axis = 1)\n",
    "\n",
    "    # voter = svm.SVC()\n",
    "\n",
    "    voter = RandomForestClassifier(n_estimators = 1000,\n",
    "                bootstrap = True,\n",
    "                n_jobs = -1, \n",
    "                min_samples_split=200,\n",
    "                oob_score = True)\n",
    "    voter.fit(all_pred, y_voter)\n",
    "\n",
    "    x_val = pickle.load(open(val_x_path, 'rb'))\n",
    "    x_val = scaler.transform(x_val)\n",
    "    y_val = pickle.load(open(val_y_path, 'rb'))\n",
    "    y_val = label_encorder.transform(y_val)\n",
    "    rf_pred = rf_clf.predict_proba(x_val)\n",
    "\n",
    "    # xgb bagging\n",
    "    # dval = xgb.DMatrix(x_val)\n",
    "    # xgb_pred = np.zeros((11880,3))\n",
    "    # # xgb_pred = []\n",
    "    # for i in range(MODEL_NUM):\n",
    "    #     model = pickle.load(open('stacking/' + str(i) + '_xgb', 'rb'))\n",
    "    #     xgb_pred += model.predict(dval, ntree_limit = model.best_ntree_limit)\n",
    "    # xgb_pred /= MODEL_NUM\n",
    "    # val_pred = np.concatenate((rf_pred, xgb_pred), axis = 1)\n",
    "\n",
    "    # xgb stacking\n",
    "    dval = xgb.DMatrix(x_val)\n",
    "    xgb_pred = []\n",
    "    for i in range(MODEL_NUM):\n",
    "        model = pickle.load(open('stacking/' + str(i) + '_xgb', 'rb'))\n",
    "        xgb_pred.append(model.predict(dval, ntree_limit = model.best_ntree_limit))\n",
    "    xgb_pred = np.concatenate(xgb_pred, axis = 1)\n",
    "    val_pred = np.concatenate((rf_pred,xgb_pred), axis = 1)\n",
    "\n",
    "    dtrain = xgb.DMatrix(all_pred, y_voter)\n",
    "    dval = xgb.DMatrix(val_pred, y_val)\n",
    "\n",
    "    # xgb_par = {\n",
    "    #         'objective' : 'multi:softprob',\n",
    "    #         'booster' : 'gbtree',\n",
    "    #         'eta' : 0.03,\n",
    "    #         'subsample' : 0.9,\n",
    "    #         'num_class' : 3,\n",
    "    #         'max_depth' : 15,\n",
    "    #         'colsample_bytree' : .6,\n",
    "    #         }\n",
    "\n",
    "    # clf = xgb.train(xgb_par, \n",
    "    #         dtrain = dtrain,\n",
    "    #         evals = [(dval, 'eval')],\n",
    "    #         num_boost_round = 1000,\n",
    "    #         early_stopping_rounds = 40,\n",
    "    #         )\n",
    "    score = voter.score(val_pred, y_val)\n",
    "    val_score.append(score)\n",
    "    print('score:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
